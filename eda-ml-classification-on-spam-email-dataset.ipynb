{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2355807,"sourceType":"datasetVersion","datasetId":1422521}],"dockerImageVersionId":30203,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"text-align: center; background-color: #649eff; color: white; padding: 14px; line-height: 1;border-radius:10px\">üìÆEDA & Classification on Spam Email Dataset</div>","metadata":{}},{"cell_type":"markdown","source":"![image](https://thumbs.dreamstime.com/b/spam-mail-printed-wooden-cube-spam-mail-printed-wooden-cubes-193211215.jpg)\n\n<cite>Image source: https://www.dreamstime.com/photos-images/spam-mail.html","metadata":{}},{"cell_type":"markdown","source":"*** ","metadata":{}},{"cell_type":"markdown","source":"> <h2> 1. About Dataset </h2>\n\n<br>\nThe dataset <b>'Spam Email'</b> contains <b>2 columns</b>, each are:\n<br>\n\n* <b>Category</b>:     Whether it is spam or ham\n\n* <b>Message</b>:      context of message","metadata":{}},{"cell_type":"markdown","source":"> <h2> 2. Notebook Objectives </h2>\n\n<br> \n<b>Goal of this notebook is to:</b><br>\n\n<br>\n\n1.üìä<b><mark>Explore</mark></b> each columns' <b><mark>distribution</mark></b> in the dataset <br>\n\n2.üìâAnalysis on <b style='color:blue'>ham messages</b> <br>\n\n3.üìàAnalysis on <b style='color:red'>spam messages</b> <br>\n\n4.0Ô∏è‚É£1Ô∏è‚É£<b style = 'color:green'>Binary Classification </b> with no tuned ML models (model comparison) and Neural Network","metadata":{}},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n> <div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n <h2>3.Table of Contents</h2>\n    \n   * [1. Load necessary libraries and dataset](#1)\n   * [2.EDA on features (length added)](#2)\n    - [2.1. Distribution of each categories](#2.1)\n    - [2.2. Length distribution of spam & ham meesage](#2.2)\n   * [3. Ham & Spam Analysis](#3)\n    - [3.1. define functions](#3.1)  \n    - [3.2. Term frequency by ham message](#3.2)\n        - [3.2.1. bar plot of ham message](#3.2.1)\n        - [3.2.2. Word cloud of ham message](#3.2.2)\n    - [3.3. Term frequency by spam message](#3.3)\n        - [3.3.1. bar plot of spam message](#3.3.1)\n        - [3.3.2. Word cloud of spam message](#3.3.2)\n   * [4. Text preprocessing for spam email detection](#4)\n        - [4.1. define preprocessing function](#4.1)  \n        - [4.2. CountVectorizer](#4.2) \n        - [4.3. tf-idf transformer](#4.3) \n        - [4.4. Train-test split](#4.4) \n   * [5. Text Classification](#5)\n       - [5.1. Define & Fit classifiers](#5.1)\n       - [5.2.ML: Compare Evaluation Matrix](#5.2)\n       - [5.3. ML: Cross Validation](#5.3)\n       - [5.4. Voting classifier Added (cv)](#5.4)\n       - [5.5. Voting Classifier: Model Evaluation](#5.5)\n       - [5.6. LSTM Model](#5.6)","metadata":{"execution":{"iopub.status.busy":"2022-07-13T07:11:17.052135Z","iopub.execute_input":"2022-07-13T07:11:17.052566Z","iopub.status.idle":"2022-07-13T07:11:17.068539Z","shell.execute_reply.started":"2022-07-13T07:11:17.052536Z","shell.execute_reply":"2022-07-13T07:11:17.066298Z"}}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px; line-height: 1;border-radius:20px\">1. Load Necessary Libraries and Dataset</div>","metadata":{}},{"cell_type":"code","source":"# for data\nimport pandas as pd\nimport numpy as np\n\n# for visualization\nimport matplotlib as mpl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\n\n\n# nltk used for NLP\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\n# Preprocessing (sklearn)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\n# Modeling\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm.sklearn import LGBMClassifier\nimport xgboost as xgb\nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\n\n# Neural Network\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, GlobalMaxPooling1D, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# scoring\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve, RocCurveDisplay\n\n# styling\nplt.style.use('ggplot')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-18T08:17:08.130075Z","iopub.execute_input":"2023-04-18T08:17:08.130785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"read dataset:","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/spam-email/spam.csv')\nmsno.matrix(df).set_title('Distribution of missing values',fontsize=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>No missing value in the dataframe</b>","metadata":{}},{"cell_type":"code","source":"print(df.shape)\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:33.35224Z","iopub.execute_input":"2023-02-01T10:23:33.353459Z","iopub.status.idle":"2023-02-01T10:23:33.367462Z","shell.execute_reply.started":"2023-02-01T10:23:33.35342Z","shell.execute_reply":"2023-02-01T10:23:33.366386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> We can notice that this dataset consists of 2 columns with 5572 rows. I'll also add extra column 'length' in the next section.</b>","metadata":{}},{"cell_type":"markdown","source":"\n\n\n<a id=\"2\"></a>\n# <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px; line-height: 1;border-radius:15px\">2.EDA on features (length added)</div>","metadata":{}},{"cell_type":"markdown","source":"<h3> Two parts in 'EDA on each categories' section, each are:</h3>\n\n* (1) Explore <b><mark>Distribution</mark></b> of each categories\n\n* (2) Explore <b style='color:red'>ham</b> & <b style='color:blue'>spam</b> message <b>length</b> distribution ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; border-radius:15px;padding: 10px; line-height: 1\">2.1. Distribution of each category</div>","metadata":{}},{"cell_type":"code","source":"category_ct = df['Category'].value_counts()\n\nfig = px.pie(values=category_ct.values, \n             names=category_ct.index, \n             color_discrete_sequence=px.colors.sequential.OrRd,\n             title= 'Pie Graph: spam or not')\nfig.update_traces(hoverinfo='label+percent', textinfo='label+value+percent', textfont_size=15,\n                  marker=dict(line=dict(color='#000000', width=2)))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:33.37043Z","iopub.execute_input":"2023-02-01T10:23:33.371177Z","iopub.status.idle":"2023-02-01T10:23:33.446151Z","shell.execute_reply.started":"2023-02-01T10:23:33.37114Z","shell.execute_reply":"2023-02-01T10:23:33.445177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Ham category accounts for 86.6% with 4825 values in the dataset. </b>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; border-radius:15px;padding: 10px; line-height: 1\">2.2. Length distribution of spam & ham meesage </div>","metadata":{}},{"cell_type":"code","source":"categories = pd.get_dummies(df[\"Category\"])\nspam_or_not = pd.concat([df, categories], axis=1)\nspam_or_not.drop('Category',axis=1,inplace=True)\n\ndf[\"length\"] = df[\"Message\"].apply(len)\n\nham = df.loc[np.where(spam_or_not['ham'] == 1)].reset_index()\nspam = df.loc[np.where(spam_or_not['ham'] == 0)].reset_index()\n\nham.drop('index',axis=1,inplace=True)\nspam.drop('index',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:33.447946Z","iopub.execute_input":"2023-02-01T10:23:33.44868Z","iopub.status.idle":"2023-02-01T10:23:33.471057Z","shell.execute_reply.started":"2023-02-01T10:23:33.448644Z","shell.execute_reply":"2023-02-01T10:23:33.470038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist_data = [ham['length'],spam['length']]\n\ngroup_labels = ['ham','spam']\n\ncolors = ['black', 'red']\n\n# Create distplot with curve_type set to 'normal'\nfig = ff.create_distplot(hist_data, group_labels, show_hist=False, colors=colors)\n\n# Add title\nfig.update_layout(title_text='Length distribution of ham and spam messages',\n                 template = 'simple_white')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:33.472747Z","iopub.execute_input":"2023-02-01T10:23:33.473422Z","iopub.status.idle":"2023-02-01T10:23:33.643406Z","shell.execute_reply.started":"2023-02-01T10:23:33.473385Z","shell.execute_reply":"2023-02-01T10:23:33.642353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <b style= 'color:red'>Spam</b> messages are mainly distributed right on 100 while \n    <b style = 'color:blue'>Ham </b>messages are distributed left on the length of 100.\n    <br>\n    <b>Thus, we can conclude as spam message tends to have more letters than hpam message</b>.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n# <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px; border-radius:20px;line-height: 1\"> 3. Ham & Spam Message Analysis </div>","metadata":{}},{"cell_type":"markdown","source":"<h3> Here, we'll explore unigrams of ham & spam messages, using (1)barplot and (2)wordcloud. </h3>","metadata":{}},{"cell_type":"markdown","source":"* (1) define <b><mark>functions</mark></b>\n* (2) Term frequency by <b style= 'color: blue'>ham</b> messages\n    - barplot\n    - word cloud\n* (3) Term frequency by <b style= 'color: red'>spam</b> messages\n    - barplot\n    - word cloud","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding: 10px;border-radius:15px; line-height: 1\"> 3.1. define functions</div>","metadata":{}},{"cell_type":"markdown","source":"<h3>Defined 8 functions, each are:</h3>\n\n* (1) <b>get_all_str()</b>:      to get all of strings from dataframe column.\n* (2) <b>get_str()</b>:          get string from list\n* (3) <b>get_word()</b>:         to get words from text, using RegexpTokenizer\n* (4) <b>stopword_list()</b>:    to add stopwords to nltp stopword list\n* (5) <b>remove_stopword()</b>:  to remove stopwords from list\n* (6) <b>Freq_df()</b>: to get dataframe from cleanwordlist\n* (7) <b>lemmatization()</b>: to lemmatize words\n\nand also visualization function:\n\n* (8) <b>Word_Cloud()</b>: plot word cloud of words","metadata":{}},{"cell_type":"code","source":"# function to get all of strings from dataframe column, and used lower function here.\ndef get_all_str(df):\n    sentence = ''\n    for i in range(len(df)):\n        sentence += df['Message'][i]\n    sentence = sentence.lower()\n    return sentence\n\ndef get_str(lst):\n    sentence = ''\n    for char in lst:\n        sentence += char+' '\n    sentence = sentence.lower()\n    return sentence\n\n# function to get words from text(string). used RegexpTokenizer\ndef get_word(text): \n    result = nltk.RegexpTokenizer(r'\\w+').tokenize(text.lower())\n#     result = result.lower()                                              \n#     result = nltk.word_tokenize(text)\n    return result\n\n# function to add stopwords to nltp stopword list.\ndef stopword_list(stop):\n    lst = stopwords.words('english')\n    for stopword in stop:\n        lst.append(stopword)\n    return lst\n\n# function to remove stopwords from list.\ndef remove_stopword(stopwords, lst):\n    stoplist = stopword_list(stopwords)\n    txt = ''\n    for idx in range(len(lst)):\n        txt += lst[idx]\n        txt += '\\n'\n    cleanwordlist = [word for word in txt.split() if word not in stoplist] \n    return cleanwordlist\n\n# function to get dataframe from cleanwordlist.\ndef Freq_df(cleanwordlist):\n    Freq_dist_nltk = nltk.FreqDist(cleanwordlist)\n    df_freq = pd.DataFrame.from_dict(Freq_dist_nltk, orient='index')\n    df_freq.columns = ['Frequency']\n    df_freq.index.name = 'Term'\n    df_freq = df_freq.sort_values(by=['Frequency'],ascending=False)\n    df_freq = df_freq.reset_index()\n    return df_freq\n\n# function to lemmatize words\ndef lemmatization(words):\n    lemm = WordNetLemmatizer()\n    tokens = [lemm.lemmatize(word) for word in words]\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:33.645102Z","iopub.execute_input":"2023-02-01T10:23:33.645767Z","iopub.status.idle":"2023-02-01T10:23:33.659268Z","shell.execute_reply.started":"2023-02-01T10:23:33.645727Z","shell.execute_reply":"2023-02-01T10:23:33.658024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Visualization functions below:</b>","metadata":{}},{"cell_type":"code","source":"# function to plot word cloud of words\ndef Word_Cloud(data, color_background, colormap, title):\n    plt.figure(figsize = (20,15))\n    wc = WordCloud(width=800, \n               height=400, \n               max_words=100,\n               colormap= colormap,\n               max_font_size=140,\n               min_font_size = 2,\n               random_state=8888, \n               background_color=color_background).generate_from_frequencies(data)\n    \n    plt.imshow(wc, interpolation='bilinear')\n    plt.title(title, fontsize=20)\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:33.663663Z","iopub.execute_input":"2023-02-01T10:23:33.666577Z","iopub.status.idle":"2023-02-01T10:23:33.67606Z","shell.execute_reply.started":"2023-02-01T10:23:33.666516Z","shell.execute_reply":"2023-02-01T10:23:33.67508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding: 10px;border-radius:15px; line-height: 1\">3.2. Term frequency by <b style = 'color: blue'>ham</b> message</div>","metadata":{}},{"cell_type":"code","source":"ham.tail(3)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:33.677447Z","iopub.execute_input":"2023-02-01T10:23:33.678283Z","iopub.status.idle":"2023-02-01T10:23:33.697517Z","shell.execute_reply.started":"2023-02-01T10:23:33.678245Z","shell.execute_reply":"2023-02-01T10:23:33.696285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>remove stopwords</b>","metadata":{}},{"cell_type":"code","source":"string = get_all_str(ham)\nwords = get_word(string)\nremoved = remove_stopword('1',words)\n# show 10 words for example\nprint(removed[:10])","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:33.70469Z","iopub.execute_input":"2023-02-01T10:23:33.70506Z","iopub.status.idle":"2023-02-01T10:23:33.996557Z","shell.execute_reply.started":"2023-02-01T10:23:33.705023Z","shell.execute_reply":"2023-02-01T10:23:33.995406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the result of last 10 words","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.1\"></a>\n### <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px;border-radius:15px; line-height: 1\">3.2.1. <b>bar plot</b> of ham message</div>","metadata":{}},{"cell_type":"code","source":"freq_df = Freq_df(removed)\ntop_10 = freq_df[:10]\n\nfig = px.bar(top_10, x = 'Term', y = 'Frequency',text = 'Frequency', color='Term',\n             color_discrete_sequence=px.colors.sequential.PuBuGn, \n             title = 'Rank of Ham Terms',\n             template = \"simple_white\"\n              )\n\nfor idx in range(len(top_10)):\n    fig.data[idx].marker.line.width = 2\n    fig.data[idx].marker.line.color = \"black\"\n    \nfig.update_traces(textposition='inside',\n                  textfont_size=11)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:33.999837Z","iopub.execute_input":"2023-02-01T10:23:34.002593Z","iopub.status.idle":"2023-02-01T10:23:34.221722Z","shell.execute_reply.started":"2023-02-01T10:23:34.002553Z","shell.execute_reply":"2023-02-01T10:23:34.220654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> 'u', '2', 'gt'</b> ranked top 3 words in ham category, which acutally means 'you', 'to'","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.2\"></a>\n### <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px; border-radius:15px;line-height: 1\">3.2.2. <b>Word cloud</b> of ham message</div>","metadata":{}},{"cell_type":"code","source":"data = dict(zip(freq_df['Term'].tolist(), freq_df['Frequency'].tolist()))\ndata = freq_df.set_index('Term').to_dict()['Frequency']\n\nham_wordcloud = Word_Cloud(data ,'white', 'seismic', 'Terms of ham message')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:34.226251Z","iopub.execute_input":"2023-02-01T10:23:34.226673Z","iopub.status.idle":"2023-02-01T10:23:35.39222Z","shell.execute_reply.started":"2023-02-01T10:23:34.226619Z","shell.execute_reply":"2023-02-01T10:23:35.390949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>above is a word cloud of ham messages, now let's explore spam message in the next section.</b>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding: 10px; border-radius:15px ;line-height: 1\">3.3. Term frequency by <b style=\"color:red\"> spam</b> message</div>\n","metadata":{}},{"cell_type":"code","source":"string = get_all_str(spam)\nwords = get_word(string)\nremoved = remove_stopword('1',words)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:35.393723Z","iopub.execute_input":"2023-02-01T10:23:35.394179Z","iopub.status.idle":"2023-02-01T10:23:35.484166Z","shell.execute_reply.started":"2023-02-01T10:23:35.394142Z","shell.execute_reply":"2023-02-01T10:23:35.483183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.3.1\"></a>\n### <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px;border-radius:15px; line-height: 1\">3.3.1. <b>bar plot</b> of spam message</div>","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize=(20,5))\n# sns.barplot(data = freq_df[:10],\n#             x = 'Term', y = 'Frequency')\nfreq_df = Freq_df(removed)\ntop_10 = freq_df[:10]\n\nfig = px.bar(top_10, x = 'Term', y = 'Frequency',text = 'Frequency',\n             color_discrete_sequence=px.colors.sequential.PuRd, \n             title = 'Rank of Spam Terms', \n             template = \"simple_white\",\n             color='Term')\n\nfor idx in range(len(top_10)):\n    fig.data[idx].marker.line.width = 2\n    fig.data[idx].marker.line.color = \"black\"\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:35.488519Z","iopub.execute_input":"2023-02-01T10:23:35.490805Z","iopub.status.idle":"2023-02-01T10:23:35.659577Z","shell.execute_reply.started":"2023-02-01T10:23:35.490767Z","shell.execute_reply":"2023-02-01T10:23:35.658637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Obviously, <u>'call'</u>, <u>'2'</u>  and <u>'free'</u> ranked top 3 frequency terms in spam messages. Those words are commonly considered to be a spam words in our daily life.</b>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.2\"></a>\n### <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px;border-radius:15px; line-height: 1\">3.3.2. <b>Word cloud</b> of spam message</div>","metadata":{}},{"cell_type":"code","source":"data = dict(zip(freq_df['Term'].tolist(), freq_df['Frequency'].tolist()))\ndata = freq_df.set_index('Term').to_dict()['Frequency']\n\nspam_wordcloud = Word_Cloud(data, 'white','seismic', 'Terms of ham message')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:35.663794Z","iopub.execute_input":"2023-02-01T10:23:35.666086Z","iopub.status.idle":"2023-02-01T10:23:36.514743Z","shell.execute_reply.started":"2023-02-01T10:23:35.666048Z","shell.execute_reply":"2023-02-01T10:23:36.513684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Additionally it is obvious that word cloud of spam category is quite <br>different from ham category, as it also involves terms such as <b style= 'color:red'>free, prize, win, awarded</b>.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px; border-radius:20px;line-height: 1\"> 4. Text preprocessing for spam email detection</div>","metadata":{}},{"cell_type":"markdown","source":"<h3>Preprocess dataframe for classification in the next section </h3>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding: 10px;border-radius:15px; line-height: 1\">4.1. define preprocessing function </div>","metadata":{}},{"cell_type":"markdown","source":"<b>function to get words from sentence, and lemmatize it with removing stopwords.</b>","metadata":{}},{"cell_type":"code","source":"def preprocess(sentence):\n    words = get_word(sentence)\n    words_ltz = lemmatization(words)\n    removed = remove_stopword('1',words_ltz)\n    return removed","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:36.516416Z","iopub.execute_input":"2023-02-01T10:23:36.517083Z","iopub.status.idle":"2023-02-01T10:23:36.526214Z","shell.execute_reply.started":"2023-02-01T10:23:36.517045Z","shell.execute_reply":"2023-02-01T10:23:36.52475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>and also replace 'ham' value into 1, 'spam' value into 0.</b>","metadata":{}},{"cell_type":"code","source":"df.replace('ham',1,inplace=True)\ndf.replace('spam',0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:36.528178Z","iopub.execute_input":"2023-02-01T10:23:36.528554Z","iopub.status.idle":"2023-02-01T10:23:36.548475Z","shell.execute_reply.started":"2023-02-01T10:23:36.528493Z","shell.execute_reply":"2023-02-01T10:23:36.547568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:36.549728Z","iopub.execute_input":"2023-02-01T10:23:36.550645Z","iopub.status.idle":"2023-02-01T10:23:36.57058Z","shell.execute_reply.started":"2023-02-01T10:23:36.550608Z","shell.execute_reply":"2023-02-01T10:23:36.569668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding: 10px;border-radius:15px; line-height: 1\">4.2. CountVectorizer</div>","metadata":{}},{"cell_type":"markdown","source":"<b><mark>CountVectorizer</mark> is used to convert text documents to a vector of term counts.</b>","metadata":{}},{"cell_type":"code","source":"vector = CountVectorizer(analyzer = preprocess)\nX = vector.fit(df['Message'])\nX_transform = X.transform(df['Message'])","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:36.571836Z","iopub.execute_input":"2023-02-01T10:23:36.57663Z","iopub.status.idle":"2023-02-01T10:23:41.977861Z","shell.execute_reply.started":"2023-02-01T10:23:36.576593Z","shell.execute_reply":"2023-02-01T10:23:41.976829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding: 10px; border-radius:15px;line-height: 1\">4.3. tf-idf transformer</div>","metadata":{}},{"cell_type":"markdown","source":"<b>Transform a count matrix to a normalized <mark>tf-idf</mark> representation.</b>","metadata":{}},{"cell_type":"code","source":"tfidf_transformer = TfidfTransformer().fit(X_transform)\nX = tfidf_transformer.transform(X_transform)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:41.982591Z","iopub.execute_input":"2023-02-01T10:23:41.984879Z","iopub.status.idle":"2023-02-01T10:23:41.998368Z","shell.execute_reply.started":"2023-02-01T10:23:41.984839Z","shell.execute_reply":"2023-02-01T10:23:41.9974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.4\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding: 10px;border-radius:15px; line-height: 1\">4.4. Train-test split</div>","metadata":{}},{"cell_type":"markdown","source":"<b>Split dataset into train set and test set, by ratio of 0.3</b>","metadata":{}},{"cell_type":"code","source":"train_X, test_X, train_y, test_y = train_test_split(X, df['Category'], test_size=0.30, random_state = 8888)    ","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:42.002486Z","iopub.execute_input":"2023-02-01T10:23:42.004714Z","iopub.status.idle":"2023-02-01T10:23:42.014342Z","shell.execute_reply.started":"2023-02-01T10:23:42.004677Z","shell.execute_reply":"2023-02-01T10:23:42.013142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px; border-radius:20px;line-height: 1\"> 5. Text Classification</div>","metadata":{}},{"cell_type":"markdown","source":"<h4>Used 5 classification models in this notebook, each are: </h4><br>\n<b style='color:blue'>RandomForestClassifier, LightGBMClassifier, XGBClassifier, SVC, CatBoostClassifier</b>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.1\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding: 10px;border-radius:15px; line-height: 1\">5.1. Define classifiers</div>","metadata":{}},{"cell_type":"markdown","source":"<h4> Here i defined non-tuned classifiers first</h4>","metadata":{}},{"cell_type":"code","source":"rfc=RandomForestClassifier(random_state=8888)\nlgbm = LGBMClassifier(boosting_type='gbdt',objective='binary',random_state=8888)\nxgbr = xgb.XGBClassifier(objective='binary:hinge',random_state=8888)\nsvc = SVC(probability=True,random_state=8888)\ncatboost = CatBoostClassifier(random_state=8888, logging_level='Silent')\n\nrfc.fit(train_X,train_y)\nlgbm.fit(train_X, train_y)\nxgbr.fit(train_X, train_y)\nsvc.fit(train_X, train_y)\ncatboost.fit(train_X,train_y,verbose=0)\n\nclassifiers = []\nclassifiers.append(svc)\nclassifiers.append(rfc)\nclassifiers.append(xgbr)\nclassifiers.append(lgbm)\nclassifiers.append(catboost)\n\nmodel_name = ['SVC', 'Random Forest', 'XGBClassifier', 'LGBMClassifier', 'CatBoostClassifier']","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:42.01882Z","iopub.execute_input":"2023-02-01T10:23:42.021079Z","iopub.status.idle":"2023-02-01T10:24:30.030613Z","shell.execute_reply.started":"2023-02-01T10:23:42.021043Z","shell.execute_reply":"2023-02-01T10:24:30.029552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.2\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding: 10px;border-radius:15px; line-height: 1\">5.2. ML: Compare Evaluation Matrix</div>","metadata":{}},{"cell_type":"markdown","source":"<h4>Get matrix score of each classifiers</h4>","metadata":{}},{"cell_type":"code","source":"accuracy_list = []\nauc_list=[]\nrecall_list = []\nf1_list = []\n\nfor classifier in classifiers :\n    y_pred=classifier.predict(test_X)\n    y_pred_proba=classifier.predict_proba(test_X)[:,1]\n    accuracy_list.append(accuracy_score(test_y,y_pred))\n    auc_list.append(roc_auc_score(test_y, y_pred_proba))\n    recall_list.append(recall_score(test_y, y_pred))\n    f1_list.append(f1_score(test_y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:24:30.032089Z","iopub.execute_input":"2023-02-01T10:24:30.032454Z","iopub.status.idle":"2023-02-01T10:24:30.899728Z","shell.execute_reply.started":"2023-02-01T10:24:30.032411Z","shell.execute_reply":"2023-02-01T10:24:30.8987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>And define plot function used for visualize scores and accuracy</h4>","metadata":{}},{"cell_type":"code","source":"def plot_model_score(model_name, accuracy_list, auc_list, recall_list, f1_list, title):\n\n    fig = go.Figure(data=[\n        go.Bar(name='Accuracy', x=model_name, y=np.round(accuracy_list,3), text=np.round(accuracy_list,3), marker_color='#97bad9'),    \n        go.Bar(name='AUC',x=model_name, y=np.round(auc_list,3), text=np.round(auc_list,3), marker_color= '#bcd6ef'), \n        go.Bar(name='Recall',   x=model_name, y=np.round(recall_list,3), text=np.round(recall_list,3), marker_color='#ebcad9'),\n        go.Bar(name='F1',       x=model_name, y=np.round(f1_list,3), text=np.round(f1_list,3), marker_color='#d5a6bd')\n    ])\n    \n    fig.update_layout(template = 'simple_white', title = title)\n    fig.update_layout(xaxis_title=\"Models\", yaxis_title=\"Score\", font = dict(size=17, family = 'Franklin Gothic'))\n    fig.update_layout(yaxis_range=[0.7,1])\n        \n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:24:30.901404Z","iopub.execute_input":"2023-02-01T10:24:30.901794Z","iopub.status.idle":"2023-02-01T10:24:30.912057Z","shell.execute_reply.started":"2023-02-01T10:24:30.901753Z","shell.execute_reply":"2023-02-01T10:24:30.910911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model_score(model_name, accuracy_list, auc_list, recall_list, f1_list,\n                 'Accuracy, AUC, Recall Score & F1 Score')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:24:30.91381Z","iopub.execute_input":"2023-02-01T10:24:30.91458Z","iopub.status.idle":"2023-02-01T10:24:30.954973Z","shell.execute_reply.started":"2023-02-01T10:24:30.914538Z","shell.execute_reply":"2023-02-01T10:24:30.954049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* <b> Here we can see that <u style = 'color: green'>SVC</u> got highest AUC score, and <u style = 'color: green'>Random Forest</u> the next.</b> <br><br>\n* <b> Consider that models may exist <u style = 'color:red'>overfitting</u>, let's see mean values of cross validation score.</b>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.3\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding: 10px; border-radius:15px;line-height: 1\">5.3. ML: Cross Validation</div>","metadata":{"execution":{"iopub.status.busy":"2022-07-13T05:53:41.525145Z","iopub.execute_input":"2022-07-13T05:53:41.525596Z","iopub.status.idle":"2022-07-13T05:53:41.536287Z","shell.execute_reply.started":"2022-07-13T05:53:41.525563Z","shell.execute_reply":"2022-07-13T05:53:41.534986Z"}}},{"cell_type":"markdown","source":"<h4> For Cross validation, i used stratifiedKFold, which returns stratified folds.</h4>","metadata":{}},{"cell_type":"code","source":"# set random_state\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=8888)\ncv_accuracy_results = []\ncv_auc_results = []\ncv_recall_results = []\ncv_f1_results = []\n\nfor classifier in classifiers:\n    cv_accuracy_results.append(cross_val_score(classifier, train_X, y = train_y, scoring = \"accuracy\", cv = kfold))\n    cv_auc_results.append(cross_val_score(classifier, train_X, y= train_y, scoring = 'roc_auc', cv = kfold))\n    cv_recall_results.append(cross_val_score(classifier, train_X, y= train_y, scoring = 'recall', cv = kfold))\n    cv_f1_results.append(cross_val_score(classifier, train_X, y= train_y, scoring = 'f1', cv = kfold))\n    \n\ncv_accuracy_means = []\ncv_auc_means = []\ncv_recall_means = []\ncv_f1_means = []\n\nfor fold in range(5):\n    cv_accuracy_means.append(cv_accuracy_results[fold].mean())\n    cv_auc_means.append(cv_auc_results[fold].mean())\n    cv_recall_means.append(cv_recall_results[fold].mean())\n    cv_f1_means.append(cv_f1_results[fold].mean())","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-01T10:24:30.956428Z","iopub.execute_input":"2023-02-01T10:24:30.957029Z","iopub.status.idle":"2023-02-01T10:37:51.181709Z","shell.execute_reply.started":"2023-02-01T10:24:30.956991Z","shell.execute_reply":"2023-02-01T10:37:51.180754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model_score(model_name, cv_accuracy_means, cv_auc_means, cv_recall_means, cv_f1_means, 'Cross Validation (5 fold)')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:37:51.183121Z","iopub.execute_input":"2023-02-01T10:37:51.183461Z","iopub.status.idle":"2023-02-01T10:37:51.220209Z","shell.execute_reply.started":"2023-02-01T10:37:51.183427Z","shell.execute_reply":"2023-02-01T10:37:51.219191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be easily seen that <b style = 'color:blue'>Random Forest, LGBMClassifier and CatBoostClassifier</b> got overall high score of each matrix.<br> Thus, i decided these three models to build an ensemble model using voting classifier.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.4\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding:10px;border-radius:15px; line-height: 1\">5.4. Voting classifier Added (cv) </div>","metadata":{}},{"cell_type":"markdown","source":"<b> As mentioned above, i selected Random Forest, LGBMClassifier and CatBoostClassifier to build a voting classifier</b>","metadata":{}},{"cell_type":"code","source":"votingC = VotingClassifier(estimators=[('light gbm', lgbm),('Random Forest', rfc),\n                                       ('Cat boost',catboost)],voting='soft')\n\nvotingC = votingC.fit(train_X, train_y)\n\nv_accuracy = cross_val_score(votingC, train_X, y = train_y, scoring = \"accuracy\", cv = kfold)\nv_auc = cross_val_score(votingC, train_X, y = train_y, scoring = \"roc_auc\", cv = kfold)\nv_recall = cross_val_score(votingC, train_X, y = train_y, scoring = \"recall\", cv = kfold)\nv_f1 = cross_val_score(votingC, train_X, y = train_y, scoring = \"f1\", cv = kfold)\n\nvotingC_accuracy_mean = v_accuracy.mean()\nvotingC_auc_mean = v_auc.mean()\nvotingC_recall_mean = v_auc.mean()\nvotingC_f1_mean = v_auc.mean()\n\nmodel_name.append('Voting Classifier')\ncv_accuracy_means.append(votingC_accuracy_mean)\ncv_auc_means.append(votingC_auc_mean)\ncv_recall_means.append(votingC_recall_mean),\ncv_f1_means.append(votingC_f1_mean)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:37:51.225702Z","iopub.execute_input":"2023-02-01T10:37:51.225991Z","iopub.status.idle":"2023-02-01T10:50:16.18417Z","shell.execute_reply.started":"2023-02-01T10:37:51.225965Z","shell.execute_reply":"2023-02-01T10:50:16.183214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model_score(model_name, cv_accuracy_means, cv_auc_means, cv_recall_means, cv_f1_means,\n                 'cross validation (5 fold), Voting Classifier Added')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:50:16.185743Z","iopub.execute_input":"2023-02-01T10:50:16.18611Z","iopub.status.idle":"2023-02-01T10:50:16.225521Z","shell.execute_reply.started":"2023-02-01T10:50:16.186073Z","shell.execute_reply":"2023-02-01T10:50:16.224442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.5\"></a>\n## <div style=\"text-align: left; background-color: #92bbff; color: white; padding: 10px; border-radius:30px; line-height: 1\">5.5. Voting Classifier: Model Evaluation</div>","metadata":{}},{"cell_type":"code","source":"voting_y_pred = votingC.predict(test_X)\nvoting_y_pred_proba=votingC.predict_proba(test_X)[:,1]","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:50:16.227327Z","iopub.execute_input":"2023-02-01T10:50:16.228039Z","iopub.status.idle":"2023-02-01T10:50:16.450172Z","shell.execute_reply.started":"2023-02-01T10:50:16.228002Z","shell.execute_reply":"2023-02-01T10:50:16.449215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_y_pred","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:50:16.451547Z","iopub.execute_input":"2023-02-01T10:50:16.451896Z","iopub.status.idle":"2023-02-01T10:50:16.458812Z","shell.execute_reply.started":"2023-02-01T10:50:16.451861Z","shell.execute_reply":"2023-02-01T10:50:16.457921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('default')\ncm = confusion_matrix(test_y, voting_y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['spam','ham'])\ndisp.plot(cmap='binary')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:50:16.460156Z","iopub.execute_input":"2023-02-01T10:50:16.460688Z","iopub.status.idle":"2023-02-01T10:50:16.694795Z","shell.execute_reply.started":"2023-02-01T10:50:16.460652Z","shell.execute_reply":"2023-02-01T10:50:16.693871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.6\"></a>\n## <div style=\"text-align: left; background-color:#92bbff; color: white; padding: 10px;border-radius:15px; line-height: 1\">5.6. LSTM Model </div>","metadata":{}},{"cell_type":"code","source":"X = df['Message']\ny = df['Category']\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state = 8888)    \nmax_len = max(max(train_X.apply(len).values),max(test_X.apply(len).values))\n\ndef fit_tokenizer(text, oov_token):\n    tokenizer = Tokenizer(oov_token = oov_token)\n    tokenizer.fit_on_texts(text)\n    return tokenizer\n\n# for sequence, padding\ndef seq_padding(sentences, tokenizer, padding, truncating, maxlen):\n    sequences = tokenizer.texts_to_sequences(sentences)    \n    pad_trunc_sequences = pad_sequences(sequences, padding = padding, maxlen = maxlen, truncating=padding)\n    return pad_trunc_sequences\n\ntokenizer = fit_tokenizer(train_X, \"<OOV>\")\n\nword_index = tokenizer.word_index\nVOCAB_SIZE = len(word_index)\n\ntrain_X = seq_padding(train_X,tokenizer, 'post', 'post',max_len)\ntest_X = seq_padding(test_X,tokenizer, 'post', 'post', max_len) \n\ndef callback(MATRIX, PATIENCE):\n    \n    callbacks = [EarlyStopping(monitor=MATRIX, \n                                patience=PATIENCE, \n                                restore_best_weights=True), \n                  ModelCheckpoint('model',\n                                  monitor=MATRIX,\n                                  save_best_only=True, \n                                  save_weights_only=True)]\n    return callbacks\n        \ndef LSTM_model(vocab_size, embedding_dim, maxlen):\n    \n    model = Sequential()\n    \n    model.add(Embedding(vocab_size+1,embedding_dim,input_length = maxlen))\n    model.add(Bidirectional(LSTM(64,kernel_regularizer=keras.regularizers.l2(0.001))))\n    \n    tf.keras.layers.GlobalAveragePooling1D()\n\n    model.add(Dense(6, activation = 'relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n    model.add(Dense(1, activation = 'sigmoid'))\n    \n    model.compile(loss = 'binary_crossentropy',\n                  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n                  metrics = ['Accuracy']) \n    \n    return model\n\nmodel = LSTM_model(VOCAB_SIZE, 100, max_len)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:50:16.69798Z","iopub.execute_input":"2023-02-01T10:50:16.698267Z","iopub.status.idle":"2023-02-01T10:50:21.995268Z","shell.execute_reply.started":"2023-02-01T10:50:16.698241Z","shell.execute_reply":"2023-02-01T10:50:21.994328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_X, train_y, epochs = 8, batch_size = 8, callbacks = callback('Accuracy', 2))\n\ntest_prediction_label = (model.predict(test_X) >= 0.5).astype(\"int32\")  \ntest_predict_proba = model.predict(test_X, verbose=0)\n\nNN_accuracy = accuracy_score(test_y, test_prediction_label)\nNN_AUC = roc_auc_score(test_y, test_prediction_label)\nNN_recall = recall_score(test_y, test_prediction_label)\nNN_f1 = f1_score(test_y, test_prediction_label)\n\nprint(NN_accuracy, NN_AUC, NN_recall, NN_f1)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:50:21.996544Z","iopub.execute_input":"2023-02-01T10:50:21.996881Z","iopub.status.idle":"2023-02-01T10:52:50.921376Z","shell.execute_reply.started":"2023-02-01T10:50:21.996847Z","shell.execute_reply":"2023-02-01T10:52:50.920337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_accuracy_means.append(NN_accuracy)\ncv_auc_means.append(NN_AUC)\ncv_recall_means.append(NN_recall),\ncv_f1_means.append(NN_f1)\nmodel_name.append('NN')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:52:50.923011Z","iopub.execute_input":"2023-02-01T10:52:50.923363Z","iopub.status.idle":"2023-02-01T10:52:50.930645Z","shell.execute_reply.started":"2023-02-01T10:52:50.923328Z","shell.execute_reply":"2023-02-01T10:52:50.929575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model_score(model_name, cv_accuracy_means, cv_auc_means, cv_recall_means, cv_f1_means,\n                 'cross validation (5 fold)')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:55:35.723489Z","iopub.execute_input":"2023-02-01T10:55:35.723877Z","iopub.status.idle":"2023-02-01T10:55:35.763202Z","shell.execute_reply.started":"2023-02-01T10:55:35.723844Z","shell.execute_reply":"2023-02-01T10:55:35.76228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}